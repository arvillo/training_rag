{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac3e361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"capital\": \"Jakarta\", \"area\": 664}\n",
      "The capital of Indonesia now is Jakarta with area 664\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class City(BaseModel):\n",
    "    capital: str\n",
    "    area: float\n",
    "\n",
    "llm = genai.Client(\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\"),\n",
    "\n",
    ")\n",
    "\n",
    "response = llm.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Apa ibukota saat ini di Indonesia dan luasnya?\",\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_schema\": City\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.text)\n",
    "print(\"The capital of Indonesia now is \" + json.loads(response.text)['capital'] + \" with area \" + str(json.loads(response.text)['area']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a9c314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DOWNLOAD\\training_rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML visualization saved to: file://d:\\DOWNLOAD\\training_rag\\chonkie.html\n"
     ]
    }
   ],
   "source": [
    "from markitdown import MarkItDown\n",
    "from chonkie import RecursiveChunker, Visualizer\n",
    "\n",
    "md = MarkItDown()\n",
    "\n",
    "source_file = \"sample pdf.pdf\"\n",
    "result = md.convert(source_file)\n",
    "\n",
    "markdown = result.text_content\n",
    "\n",
    "chunker = RecursiveChunker.from_recipe(\"markdown\", lang=\"en\")\n",
    "chunks = chunker.chunk(markdown)\n",
    "\n",
    "viz = Visualizer()\n",
    "viz.save(\"chonkie.html\", chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f48d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len,\n",
    "    separators = [\"\\n\\n\",\"\\n\",\" \", \"\"]\n",
    ")\n",
    "\n",
    "load_docs = PyPDFLoader(\"sample pdf.pdf\")\n",
    "pages = load_docs.load()\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "#print(len(pages))\n",
    "#print(len(chunks))\n",
    "#print(chunks[0].page_content)\n",
    "#print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86249ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "try:\n",
    "    client = chromadb.PersistentClient('db/workshop_1')\n",
    "    client.delete_collection(name=\"gemini_demo\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    model_name=\"gemini-embedding-001\"\n",
    ")\n",
    "\n",
    "client = chromadb.PersistentClient('db/workshop_1')\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name = \"gemini_demo\",\n",
    "    embedding_function = google_ef\n",
    ")\n",
    "\n",
    "for filename in os.listdir(\"D:/DOWNLOAD/training_rag\"):\n",
    "    if filename.endswith(\"pdf\"):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = 500,\n",
    "            chunk_overlap = 100,\n",
    "            length_function = len,\n",
    "            separators = [\"\\n\\n\",\"\\n\",\" \", \"\"]\n",
    "        )\n",
    "        file = os.path.join(\"D:/DOWNLOAD/training_rag\", filename)\n",
    "        load_docs = PyPDFLoader(file)\n",
    "        pages = load_docs.load()\n",
    "\n",
    "        chunks = text_splitter.split_documents(pages)\n",
    "        collection.add(\n",
    "            ids = [f\"chunk_{i}\" for i, _ in enumerate(chunks)],\n",
    "            documents = [doc.page_content for doc in chunks],\n",
    "            metadatas = [doc.metadata for doc in chunks]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67d71046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Financial Ruin: The cost of alcohol, combined with potential job loss due to \n",
      "poor performance or legal fees (DUI), can lead to bankruptcy. \n",
      " Legal Consequences: Arrests for public intoxication, disorderly conduct, or \n",
      "driving under the inﬂuence (DUI) leave permanent criminal records. \n",
      " \n",
      "5. Summary of Statistics \n",
      " Fatalities: Alcohol contributes to more than 3 million deaths globally each year \n",
      "(WHO). \n",
      " Youth Risk: Alcohol is a leading factor in death for people aged 15–49.\n"
     ]
    }
   ],
   "source": [
    "collection = client.get_collection(\"gemini_demo\", embedding_function=google_ef)\n",
    "\n",
    "query = \"Di binus ada makanan apa aja?\"\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts = query,\n",
    "    n_results = 5\n",
    ")\n",
    "\n",
    "print(results['documents'][0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "401d4202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot answer this based on the provided context.\n"
     ]
    }
   ],
   "source": [
    "context = \"\\n\".join(results['documents'][0])\n",
    "\n",
    "prompt = f\"\"\"Use the following context to answer the question. If you cannot answer the question based on the context, say \"I cannot answer this based on the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\"\"\"\n",
    "\n",
    "llm = genai.Client(\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "response = llm.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = prompt,\n",
    "    config = {\n",
    "        \"system_instruction\": \"You are helpful assistant that answer the question based on provided context\",\n",
    "        \"temperature\":\"0\" #0 - 1\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0fb575",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "519a8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "db = duckdb.connect(\"db/workshop_db\")\n",
    "table_name = 'alcohol_price'\n",
    "\n",
    "if not db.sql(f\"SELECT * FROM information_schema.tables WHERE table_name = '{table_name}'\").fetchone():\n",
    "    db.sql(f\"CREATE TABLE {table_name} AS SELECT * FROM read_csv_auto('sample csv.csv')\")\n",
    "\n",
    "df = db.sql(f\"DESCRIBE {table_name}\")\n",
    "\n",
    "query = \"Alcohol yang paling mahal dong apa ya?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    You are a helpful assistant that can generate SQL queries that answer the question about the data.\n",
    "    Please do select it from table {table_name}\n",
    "    Also do notes that the column of the {table_name} were:\n",
    "    {df}\n",
    "\n",
    "    Here are the question for you to answer: {query}\n",
    "\"\"\"\n",
    "\n",
    "response = llm.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config = {\n",
    "        \"system_instruction\": \"You are a helpful assistant that can generate SQL queries. Only return the SQL and nothing more\",\n",
    "        \"temperature\": \"0\"\n",
    "    }\n",
    ")\n",
    "\n",
    "sql_query = response.text\n",
    "sql_query = sql_query.replace(\"```sql\",\"\").replace(\"```\",\"\")\n",
    "results = db.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "737ba1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alkohol paling mahal apa\n",
      "text_2_db\n"
     ]
    },
    {
     "ename": "ConnectionException",
     "evalue": "Connection Error: Connection has already been closed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 143\u001b[39m\n\u001b[32m    140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m retrieve_query(rewrite)\n\u001b[32m    142\u001b[39m res = run_agentic_rag(\u001b[33m\"\u001b[39m\u001b[33mAlokohol paling mahal apa\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mConnectionException\u001b[39m: Connection Error: Connection has already been closed"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379)\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.getenv('GEMINI_API_KEY'),\n",
    ")\n",
    "\n",
    "def get_history(user_id):\n",
    "    messages = r.lrange(f\"chat:{user_id}\",0,-1)\n",
    "    return [json.loads(m.decode()) for m in messages]\n",
    "\n",
    "def add_history(user_id, role, messages):\n",
    "    r.rpush(f\"chat:{user_id}\", json.dumps({\"role\": role, \"content\": messages}))\n",
    "\n",
    "def rewrite_query(query, conversations: list[dict]):\n",
    "    history_context = \"\\n\".join([f\"{conv['role']}: {conv['content']}\" for conv in conversations])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    The conversation context is\n",
    "    {history_context}\n",
    "    Query:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemini-2.5-flash\",\n",
    "        contents = prompt,\n",
    "        config = {\n",
    "            \"system_instruction\": \"You are helpful assistant that can rewrite queries to be more accurate. Always put the name of the person or produt if the had mentioned.\",\n",
    "            \"temperature\": \"0\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def get_rag_answer(results, query, model, model_name=\"gemini-2.5-flash\", temperature=0):\n",
    "    context = \"\\n\".join(results)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Use the following context to answer the question.\n",
    "    If you cannot answer the question based on the context, please answer \"I can't answer it based on the following context\".\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Query:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model = f\"{model_name}\",\n",
    "        contents = prompt,\n",
    "        config = {\n",
    "            \"system_instruction\": \"You are helpful assistant that can rewrite queries to be more accurate. Always put the name of the person or produt if the had mentioned.\",\n",
    "            \"temperature\": f\"{temperature}\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def retrieve_query(query):\n",
    "    results = collection.query(\n",
    "        query_texts = query,\n",
    "        n_results = 3\n",
    "    )\n",
    "\n",
    "    return get_rag_answer(results['documents'][0], query, client)\n",
    "\n",
    "def text_2_db(query):\n",
    "    db = duckdb.connect(\"db/workshop_db\")\n",
    "    table_name = 'alcohol_price'\n",
    "\n",
    "    df = db.sql(f\"DESCRIBE {table_name}\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are a helpful assistant that can generate SQL queries that answer the question about the data.\n",
    "        Please do select it from table {table_name}\n",
    "        Also do notes that the column of the {table_name} were:\n",
    "        {df}\n",
    "\n",
    "        Here are the question for you to answer: {query}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt,\n",
    "        config = {\n",
    "            \"system_instruction\": \"You are a helpful assistant that can generate SQL queries. Only return the SQL and nothing more\",\n",
    "            \"temperature\": \"0\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    sql_query = response.text\n",
    "    sql_query = sql_query.replace(\"```sql\",\"\").replace(\"```\",\"\")\n",
    "\n",
    "    res = db.sql(sql_query)\n",
    "    return res\n",
    "\n",
    "def classify_intent(query):\n",
    "    prompt = f\"\"\"\n",
    "    Decide if the query needs a vector search or text 2 sql search.print\n",
    "    If the user is asking for specific data which is usually like a numerical operation like greater than, less than, etc. then it is a text 2 sql search. Similary if they are asking for details mentioning a specific field name like find me details of employee with id 123 then it is a text 2 sql search.\n",
    "    If the user query is a general, hi, hello, etc. then it is a General intent.\n",
    "    For everything else, it is a vector search.\n",
    "\n",
    "    Here are the list of tables in the database:\n",
    "    {db.sql(\"SELECT table_name FROM information_schema.tables\").fetchall()}\n",
    "\n",
    "    here is the list of pdf documents we have ingested in vector db:\n",
    "    \"sample pdf.pdf\" regarding the danger of alcohol\n",
    "    \"sample pdf 1.pdf\" regarding pros and cons of alcohol\n",
    "    \"sample pdf 2.pdf\" regarding health concious guide of alcohol\n",
    "\n",
    "    The query is: {query}\n",
    "    Only return the Intent as a string. It will be either \"vector\" or \"text_2_db\" or \"general\".\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents = prompt,\n",
    "        config = {\n",
    "            \"temperature\": \"0\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def run_agentic_rag(query, user_id):\n",
    "    add_history(user_id, \"user\", query)\n",
    "    rewrite = rewrite_query(query, get_history(user_id))\n",
    "    print(rewrite)\n",
    "    intent = classify_intent(rewrite)\n",
    "    print(intent)\n",
    "    if intent == \"general\":\n",
    "        return \"No intention, can't answer question\"\n",
    "    elif intent == \"text_2_db\":\n",
    "        return text_2_db(rewrite)\n",
    "    elif intent == \"vector\":\n",
    "        return retrieve_query(rewrite)\n",
    "\n",
    "res = run_agentic_rag(\"Alokohol paling mahal apa\", \"a\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
